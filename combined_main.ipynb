{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual-Head Oral Pathology Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `configs/config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Base Paths (Updated for local usage or specific environment if needed)\n",
    "# NOTE: Update BASE_PATH to match your actual data location\n",
    "BASE_PATH = '/workspace'\n",
    "\n",
    "# Dataset 1 Paths\n",
    "DS1_ORIGINAL_BENIGN = os.path.join(BASE_PATH, 'Dataset 1', 'original_data', 'benign_lesions')\n",
    "DS1_ORIGINAL_MALIGNANT = os.path.join(BASE_PATH, 'Dataset 1', 'original_data', 'malignant_lesions')\n",
    "\n",
    "# Dataset 2 Paths\n",
    "DS2_TRAINING = os.path.join(BASE_PATH, 'Dataset 2 ', 'Training')\n",
    "DS2_VALIDATION = os.path.join(BASE_PATH, 'Dataset 2 ', 'Validation')\n",
    "DS2_TESTING = os.path.join(BASE_PATH, 'Dataset 2 ', 'Testing')\n",
    "\n",
    "# Dataset configuration\n",
    "DS2_CLASSES = ['CaS', 'CoS', 'Gum', 'MC', 'OC', 'OLP', 'OT']\n",
    "MALIGNANT_SUBTYPES = ['MC', 'OC', 'CaS']\n",
    "NUM_SUBTYPES = len(DS2_CLASSES)\n",
    "\n",
    "# Model configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 128  \n",
    "NUM_WORKERS = 8  \n",
    "# BACKBONE Options:\n",
    "# 'resnet50'        - ResNet50 (Default)\n",
    "# 'densenet121'     - DenseNet121\n",
    "# 'convnext_tiny'   - ConvNeXt Tiny\n",
    "# 'swin_t'          - Swin Transformer Tiny\n",
    "# 'efficientnet_b0' - EfficientNet B0\n",
    "# 'efficientnet_v2b2' - EfficientNet V2-B2\n",
    "# 'efficientnet_v2b3' - EfficientNet V2-B3\n",
    "# 'efficientnet_v2s'  - EfficientNet V2-S\n",
    "BACKBONE = 'swin_t'\n",
    "DROPOUT = 0.5\n",
    "USE_PRETRAINED = False  \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Training configuration\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_EPOCHS = 200\n",
    "SEED = 42\n",
    "\n",
    "# Learning Rate Scheduler configuration\n",
    "SCHEDULER_TYPE = 'cosine'  \n",
    "SCHEDULER_PATIENCE = 5  \n",
    "SCHEDULER_FACTOR = 0.5  \n",
    "SCHEDULER_STEP_SIZE = 10  \n",
    "SCHEDULER_GAMMA = 0.95  \n",
    "\n",
    "# Early Stopping configuration\n",
    "EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 15  \n",
    "EARLY_STOPPING_MIN_DELTA = 1e-4  \n",
    "\n",
    "# Paths for saving results\n",
    "SAVE_DIR = os.path.join(BASE_PATH, 'results', BACKBONE)\n",
    "BEST_MODEL_PATH = os.path.join(SAVE_DIR, 'best_model.pth')\n",
    "HISTORY_PLOT_PATH = os.path.join(SAVE_DIR, 'training_history.png')\n",
    "CONFUSION_MATRIX_PATH = os.path.join(SAVE_DIR, 'confusion_matrices.png')\n",
    "\n",
    "# Ensure save directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `utils/common.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the current device.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    return device\n",
    "\n",
    "def check_paths_exist(paths_list):\n",
    "    \"\"\"Verify that all paths in the list exist.\"\"\"\n",
    "    all_exist = True\n",
    "    print(\"\\nChecking paths...\")\n",
    "    for path in paths_list:\n",
    "        exists = os.path.exists(path)\n",
    "        status = \"✓\" if exists else \"✗\"\n",
    "        print(f\"{status} {path}\")\n",
    "        if not exists:\n",
    "            all_exist = False\n",
    "    return all_exist\n",
    "\n",
    "def count_images_in_folder(folder):\n",
    "    \"\"\"Count image files in a folder.\"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        return 0\n",
    "    extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff']\n",
    "    count = 0\n",
    "    for ext in extensions:\n",
    "        count += len(glob(os.path.join(folder, ext)))\n",
    "        count += len(glob(os.path.join(folder, ext.upper())))\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `utils/evaluation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    results = {\n",
    "        'preds_binary': [], 'targets_binary': [],\n",
    "        'preds_subtype': [], 'targets_subtype': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets_b, targets_s in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            pred_b, pred_s = model(images)\n",
    "            \n",
    "            results['preds_binary'].extend(torch.argmax(pred_b, dim=1).cpu().numpy())\n",
    "            results['targets_binary'].extend(targets_b.numpy())\n",
    "            \n",
    "            mask = targets_s != -1\n",
    "            if mask.sum() > 0:\n",
    "                results['preds_subtype'].extend(torch.argmax(pred_s[mask], dim=1).cpu().numpy())\n",
    "                results['targets_subtype'].extend(targets_s[mask].numpy())\n",
    "                \n",
    "    return {k: np.array(v) for k, v in results.items()}\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(title)\n",
    "\n",
    "def predict_single_image(model, image_path, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_b, pred_s = model(tensor)\n",
    "        prob_b = torch.softmax(pred_b, dim=1)[0]\n",
    "        prob_s = torch.softmax(pred_s, dim=1)[0]\n",
    "        \n",
    "        idx_b = torch.argmax(prob_b).item()\n",
    "        idx_s = torch.argmax(prob_s).item()\n",
    "    \n",
    "    return {\n",
    "        'binary': ('Malignant' if idx_b == 1 else 'Benign', prob_b[idx_b].item()),\n",
    "        'subtype': (DS2_CLASSES[idx_s], prob_s[idx_s].item())\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `data/transforms.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `data/dataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class OralPathologyDataset(Dataset):\n",
    "    \"\"\"Union Dataset for Dual-Head Multi-Task Learning.\"\"\"\n",
    "    def __init__(self, image_paths, labels_binary, labels_subtype, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels_binary = labels_binary\n",
    "        self.labels_subtype = labels_subtype\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels_binary[idx], self.labels_subtype[idx]\n",
    "\n",
    "def get_image_files(folder):\n",
    "    if not os.path.exists(folder): return []\n",
    "    extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff']\n",
    "    files = []\n",
    "    for ext in extensions:\n",
    "        files.extend(glob(os.path.join(folder, ext)))\n",
    "        files.extend(glob(os.path.join(folder, ext.upper())))\n",
    "    return files\n",
    "\n",
    "def load_dataset1_split(split='train', test_size=0.10, val_size=0.10, random_state=42):\n",
    "    \"\"\"\n",
    "    Fixed split with properly separated train/val/test sets.\n",
    "    Test set is completely held out and should ONLY be used for final evaluation.\n",
    "    \"\"\"\n",
    "    benign_paths = get_image_files(DS1_ORIGINAL_BENIGN)\n",
    "    malignant_paths = get_image_files(DS1_ORIGINAL_MALIGNANT)\n",
    "    all_paths = benign_paths + malignant_paths\n",
    "    all_binary = [0] * len(benign_paths) + [1] * len(malignant_paths)\n",
    "    all_subtype = [-1] * len(all_paths)\n",
    "    \n",
    "    # First split: separate test set (held out completely)\n",
    "    temp_paths, test_paths, temp_bin, test_bin, temp_sub, test_sub = train_test_split(\n",
    "        all_paths, all_binary, all_subtype, test_size=test_size, random_state=random_state, stratify=all_binary\n",
    "    )\n",
    "    \n",
    "    # Second split: divide remaining into train and validation\n",
    "    val_size_adj = val_size / (1 - test_size)\n",
    "    train_paths, val_paths, train_bin, val_bin, train_sub, val_sub = train_test_split(\n",
    "        temp_paths, temp_bin, temp_sub, test_size=val_size_adj, random_state=random_state, stratify=temp_bin\n",
    "    )\n",
    "    \n",
    "    if split == 'train': return train_paths, train_bin, train_sub\n",
    "    elif split == 'val': return val_paths, val_bin, val_sub\n",
    "    else: return test_paths, test_bin, test_sub\n",
    "\n",
    "def load_dataset2_split(split='train', test_size=0.20, val_size=0.20, random_state=42):\n",
    "    \"\"\"\n",
    "    Load Dataset 2 with proper train/val/test split.\n",
    "    MERGES Training + Validation folders, then splits properly to avoid\n",
    "    the suspicious pre-made Testing folder with identical distributions.\n",
    "    \"\"\"\n",
    "    image_paths, labels_binary, labels_subtype = [], [], []\n",
    "    \n",
    "    # Merge Training + Validation folders (ignore the suspicious Testing folder)\n",
    "    for base_path in [DS2_TRAINING, DS2_VALIDATION]:\n",
    "        for idx, subtype in enumerate(DS2_CLASSES):\n",
    "            subtype_path = os.path.join(base_path, subtype)\n",
    "            imgs = get_image_files(subtype_path)\n",
    "            image_paths.extend(imgs)\n",
    "            labels_subtype.extend([idx] * len(imgs))\n",
    "            labels_binary.extend([1 if subtype in MALIGNANT_SUBTYPES else 0] * len(imgs))\n",
    "    \n",
    "    # Now split this merged data properly\n",
    "    # First split: separate test set (held out completely)\n",
    "    temp_paths, test_paths, temp_bin, test_bin, temp_sub, test_sub = train_test_split(\n",
    "        image_paths, labels_binary, labels_subtype, \n",
    "        test_size=test_size, random_state=random_state, stratify=labels_subtype\n",
    "    )\n",
    "    \n",
    "    # Second split: divide remaining into train and validation\n",
    "    val_size_adj = val_size / (1 - test_size)\n",
    "    train_paths, val_paths, train_bin, val_bin, train_sub, val_sub = train_test_split(\n",
    "        temp_paths, temp_bin, temp_sub, \n",
    "        test_size=val_size_adj, random_state=random_state, stratify=temp_sub\n",
    "    )\n",
    "    \n",
    "    if split == 'train': \n",
    "        return train_paths, train_bin, train_sub\n",
    "    elif split == 'val': \n",
    "        return val_paths, val_bin, val_sub\n",
    "    else: \n",
    "        return test_paths, test_bin, test_sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `models/architecture.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "# Map user-friendly names to timm model names\n",
    "BACKBONE_MAP = {\n",
    "    'resnet50': 'resnet50',\n",
    "    'densenet121': 'densenet121',\n",
    "    'convnext_tiny': 'convnext_tiny',\n",
    "    'swin_t': 'swin_tiny_patch4_window7_224',\n",
    "    'efficientnet_b0': 'efficientnet_b0',\n",
    "    'efficientnet_v2b2': 'tf_efficientnetv2_b2',\n",
    "    'efficientnet_v2b3': 'tf_efficientnetv2_b3',\n",
    "    'efficientnet_v2s': 'tf_efficientnetv2_s',\n",
    "}\n",
    "\n",
    "class MultiTaskOralClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-Head Multi-Task Model for Oral Pathology Classification.\n",
    "    Shared Backbone with Two Independent Parallel Heads.\n",
    "    Uses 'timm' for flexible backbone selection.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone=None, num_subtypes=None, dropout=None, pretrained=None):\n",
    "        super(MultiTaskOralClassifier, self).__init__()\n",
    "        \n",
    "        # Resolve config with arguments or defaults\n",
    "        self.backbone_name = backbone if backbone else BACKBONE\n",
    "        self.num_subtypes = num_subtypes if num_subtypes is not None else NUM_SUBTYPES\n",
    "        self.dropout_val = dropout if dropout is not None else DROPOUT\n",
    "        self.use_pretrained = pretrained if pretrained is not None else USE_PRETRAINED\n",
    "        \n",
    "        # Determine timm model name\n",
    "        if self.backbone_name not in BACKBONE_MAP:\n",
    "            raise ValueError(f\"Unsupported backbone: {self.backbone_name}. Supported: {list(BACKBONE_MAP.keys())}\")\n",
    "            \n",
    "        timm_model_name = BACKBONE_MAP[self.backbone_name]\n",
    "        print(f\"Initializing {self.backbone_name} ({timm_model_name}) with pretrained={self.use_pretrained}\")\n",
    "        \n",
    "        # Initialize backbone using timm\n",
    "        self.backbone = timm.create_model(\n",
    "            timm_model_name, \n",
    "            pretrained=self.use_pretrained, \n",
    "            num_classes=0\n",
    "        )\n",
    "        \n",
    "        # Automatically get the number of output features from the backbone\n",
    "        num_features = self.backbone.num_features\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout_val)\n",
    "        \n",
    "        # Head 1: Binary Classification (Malignant vs Benign)\n",
    "        self.head_binary = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_val),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "        \n",
    "        # Head 2: Subtype Classification\n",
    "        self.head_subtype = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_val),\n",
    "            nn.Linear(512, self.num_subtypes)\n",
    "        )\n",
    "        \n",
    "        print(f\"Model initialized with {self.backbone_name} backbone (features={num_features})\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = self.dropout_layer(features)\n",
    "        \n",
    "        out_binary = self.head_binary(features)\n",
    "        out_subtype = self.head_subtype(features)\n",
    "        \n",
    "        return out_binary, out_subtype\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Backbone frozen.\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Backbone unfrozen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `models/loss.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for Multi-Task Learning.\n",
    "    Loss 1 (Binary): CrossEntropyLoss for all samples\n",
    "    Loss 2 (Subtype): CrossEntropyLoss with ignore_index=-1 (Masking Trick)\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_binary=1.0, weight_subtype=1.0):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.weight_binary = weight_binary\n",
    "        self.weight_subtype = weight_subtype\n",
    "        \n",
    "        self.criterion_binary = nn.CrossEntropyLoss()\n",
    "        self.criterion_subtype = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    def forward(self, pred_binary, pred_subtype, target_binary, target_subtype):\n",
    "        # Binary loss (all samples contribute)\n",
    "        loss_binary = self.criterion_binary(pred_binary, target_binary)\n",
    "        \n",
    "        # Subtype loss (only DS2 samples contribute via ignore_index)\n",
    "        loss_subtype = self.criterion_subtype(pred_subtype, target_subtype)\n",
    "        \n",
    "        # Handle NaN if batch has only DS1 samples\n",
    "        if torch.isnan(loss_subtype):\n",
    "            loss_subtype = torch.tensor(0.0, device=pred_binary.device)\n",
    "        \n",
    "        total_loss = (self.weight_binary * loss_binary) + (self.weight_subtype * loss_subtype)\n",
    "        \n",
    "        return total_loss, loss_binary, loss_subtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `engine/trainer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, running_loss_b, running_loss_s = 0.0, 0.0, 0.0\n",
    "    all_preds_b, all_targets_b = [], []\n",
    "    all_preds_s, all_targets_s = [], []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for images, targets_b, targets_s in pbar:\n",
    "        images, targets_b, targets_s = images.to(device), targets_b.to(device), targets_s.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_b, pred_s = model(images)\n",
    "        loss, loss_b, loss_s = criterion(pred_b, pred_s, targets_b, targets_s)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_loss_b += loss_b.item()\n",
    "        running_loss_s += loss_s.item() if not torch.isnan(loss_s) else 0\n",
    "        \n",
    "        preds_b = torch.argmax(pred_b, dim=1)\n",
    "        all_preds_b.extend(preds_b.cpu().numpy())\n",
    "        all_targets_b.extend(targets_b.cpu().numpy())\n",
    "        \n",
    "        mask = targets_s != -1\n",
    "        if mask.sum() > 0:\n",
    "            preds_s = torch.argmax(pred_s[mask], dim=1)\n",
    "            all_preds_s.extend(preds_s.cpu().numpy())\n",
    "            all_targets_s.extend(targets_s[mask].cpu().numpy())\n",
    "            \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_loss_b = running_loss_b / len(train_loader)\n",
    "    avg_loss_s = running_loss_s / len(train_loader)\n",
    "    acc_b = accuracy_score(all_targets_b, all_preds_b)\n",
    "    acc_s = accuracy_score(all_targets_s, all_preds_s) if all_targets_s else 0.0\n",
    "    \n",
    "    return avg_loss, avg_loss_b, avg_loss_s, acc_b, acc_s\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds_b, all_targets_b = [], []\n",
    "    all_preds_s, all_targets_s = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets_b, targets_s in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images, targets_b, targets_s = images.to(device), targets_b.to(device), targets_s.to(device)\n",
    "            pred_b, pred_s = model(images)\n",
    "            loss, _, _ = criterion(pred_b, pred_s, targets_b, targets_s)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            preds_b = torch.argmax(pred_b, dim=1)\n",
    "            all_preds_b.extend(preds_b.cpu().numpy())\n",
    "            all_targets_b.extend(targets_b.cpu().numpy())\n",
    "            \n",
    "            mask = targets_s != -1\n",
    "            if mask.sum() > 0:\n",
    "                preds_s = torch.argmax(pred_s[mask], dim=1)\n",
    "                all_preds_s.extend(preds_s.cpu().numpy())\n",
    "                all_targets_s.extend(targets_s[mask].cpu().numpy())\n",
    "    \n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    acc_b = accuracy_score(all_targets_b, all_preds_b)\n",
    "    acc_s = accuracy_score(all_targets_s, all_preds_s) if all_targets_s else 0.0\n",
    "    \n",
    "    return avg_loss, acc_b, acc_s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training script - Does NOT evaluate on test set.\n",
    "Test set should only be evaluated once at the very end using evaluate_final.py\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def main():\n",
    "    set_seed()\n",
    "    device = get_device()\n",
    "    \n",
    "    # Argument Parsing\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Train Oral Pathology Model')\n",
    "    parser.add_argument('--backbone', type=str, default=BACKBONE, help='Backbone model name')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    current_backbone = args.backbone\n",
    "    print(f\"Using Backbone: {current_backbone}\")\n",
    "    \n",
    "    # Update paths if backbone changed from config default\n",
    "    import os\n",
    "    from configs import config\n",
    "    \n",
    "    # Recalculate paths based on the chosen backbone\n",
    "    # Note: We use config.BASE_PATH to ensure we are relative to the workspace root\n",
    "    current_save_dir = os.path.join(config.BASE_PATH, 'results', current_backbone)\n",
    "    current_best_model_path = os.path.join(current_save_dir, 'best_model.pth')\n",
    "    \n",
    "    os.makedirs(current_save_dir, exist_ok=True)\n",
    "    print(f\"Results will be saved to: {current_save_dir}\")\n",
    "    \n",
    "    # 1. Load Datasets - ONLY TRAIN AND VAL (NO TEST!)\n",
    "    print(\"Loading datasets...\")\n",
    "    print(\" Test set is NOT loaded during training to prevent data leakage!\")\n",
    "    \n",
    "    # Dataset 1 (Binary only)\n",
    "    d1_train_p, d1_train_b, d1_train_s = load_dataset1_split('train')\n",
    "    d1_val_p, d1_val_b, d1_val_s = load_dataset1_split('val')\n",
    "    \n",
    "    # Dataset 2 (Both labels) - now properly split from merged Training+Validation\n",
    "    d2_train_p, d2_train_b, d2_train_s = load_dataset2_split('train')\n",
    "    d2_val_p, d2_val_b, d2_val_s = load_dataset2_split('val')\n",
    "    \n",
    "    # Combine\n",
    "    train_paths = d1_train_p + d2_train_p\n",
    "    train_binary = d1_train_b + d2_train_b\n",
    "    train_subtype = d1_train_s + d2_train_s\n",
    "    \n",
    "    val_paths = d1_val_p + d2_val_p\n",
    "    val_binary = d1_val_b + d2_val_b\n",
    "    val_subtype = d1_val_s + d2_val_s\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_ds = OralPathologyDataset(train_paths, train_binary, train_subtype, transform=train_transform)\n",
    "    val_ds = OralPathologyDataset(val_paths, val_binary, val_subtype, transform=val_transform)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    print(f\"Train images: {len(train_ds)}\")\n",
    "    print(f\"Val images: {len(val_ds)}\")\n",
    "    print(f\"Test images: Not loaded (use evaluate_final.py after training)\")\n",
    "    \n",
    "    # 2. Model Setup\n",
    "    model = MultiTaskOralClassifier(backbone=current_backbone).to(device)\n",
    "    criterion = MultiTaskLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Setup scheduler based on configuration\n",
    "    if SCHEDULER_TYPE == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "        print(f\"Scheduler: CosineAnnealingLR\")\n",
    "    elif SCHEDULER_TYPE == 'plateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=SCHEDULER_FACTOR, \n",
    "                                                         patience=SCHEDULER_PATIENCE, verbose=True)\n",
    "        print(f\"Scheduler: ReduceLROnPlateau (patience={SCHEDULER_PATIENCE}, factor={SCHEDULER_FACTOR})\")\n",
    "    elif SCHEDULER_TYPE == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_FACTOR)\n",
    "        print(f\"Scheduler: StepLR (step_size={SCHEDULER_STEP_SIZE}, gamma={SCHEDULER_FACTOR})\")\n",
    "    elif SCHEDULER_TYPE == 'exponential':\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=SCHEDULER_GAMMA)\n",
    "        print(f\"Scheduler: ExponentialLR (gamma={SCHEDULER_GAMMA})\")\n",
    "    else:\n",
    "        scheduler = None\n",
    "        print(\"No scheduler used\")\n",
    "    \n",
    "    # 3. Training Loop\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc_b': [], 'val_acc_s': [], 'lr': []}\n",
    "    \n",
    "    if EARLY_STOPPING:\n",
    "        print(f\"Early stopping enabled (patience={EARLY_STOPPING_PATIENCE}, min_delta={EARLY_STOPPING_MIN_DELTA})\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} | LR: {current_lr:.2e}\")\n",
    "        \n",
    "        t_loss, t_loss_b, t_loss_s, t_acc_b, t_acc_s = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        v_loss, v_acc_b, v_acc_s = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Step scheduler\n",
    "        if scheduler is not None:\n",
    "            if SCHEDULER_TYPE == 'plateau':\n",
    "                scheduler.step(v_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Track history\n",
    "        history['train_loss'].append(t_loss)\n",
    "        history['val_loss'].append(v_loss)\n",
    "        history['val_acc_b'].append(v_acc_b)\n",
    "        history['val_acc_s'].append(v_acc_s)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        print(f\"Train Loss: {t_loss:.4f} (B: {t_loss_b:.4f}, S: {t_loss_s:.4f}) | Acc B: {t_acc_b:.4f}, S: {t_acc_s:.4f}\")\n",
    "        print(f\"Val Loss: {v_loss:.4f} | Acc B: {v_acc_b:.4f}, S: {v_acc_s:.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if v_loss < (best_loss - EARLY_STOPPING_MIN_DELTA):\n",
    "            best_loss = v_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_loss = v_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), current_best_model_path)\n",
    "            print(\"✓ Best model saved\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve > 0:\n",
    "                print(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if EARLY_STOPPING and epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"No improvement for {EARLY_STOPPING_PATIENCE} consecutive epochs\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total epochs: {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
    "    print(f\"Model saved to: {current_best_model_path}\")\n",
    "    if EARLY_STOPPING and epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"Stopped early due to no improvement\")\n",
    "    print(\"\\n  IMPORTANT: To evaluate on test set, run:\")\n",
    "    print(\"   python evaluate_final.py\")\n",
    "    print(\"\\nThis ensures test set is only used ONCE for final evaluation.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `evaluate_final.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FINAL TEST SET EVALUATION\n",
    " Run this ONLY ONCE after training is complete!\n",
    "Multiple runs on the test set lead to overfitting through hyperparameter tuning.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" FINAL TEST SET EVALUATION - USE ONLY ONCE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"This script evaluates on the held-out test set.\")\n",
    "    print(\"Running this multiple times compromises the validity of results.\\n\")\n",
    "    \n",
    "    # Argument Parsing\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Evaluate Oral Pathology Model')\n",
    "    parser.add_argument('--backbone', type=str, default=BACKBONE, help='Backbone model name')\n",
    "    parser.add_argument('--no-confirm', action='store_true', help='Skip confirmation prompt')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    current_backbone = args.backbone\n",
    "    print(f\"Using Backbone: {current_backbone}\")\n",
    "\n",
    "    if not args.no_confirm:\n",
    "        response = input(\"Continue with test evaluation? (yes/no): \")\n",
    "        if response.lower() not in ['yes', 'y']:\n",
    "            print(\"Evaluation cancelled.\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Skipping confirmation prompt (--no-confirm used).\")\n",
    "    \n",
    "    # Update paths\n",
    "    import os\n",
    "    from configs import config\n",
    "    \n",
    "    # Recalculate paths based on the chosen backbone\n",
    "    current_save_dir = os.path.join(config.BASE_PATH, 'results', current_backbone)\n",
    "    current_best_model_path = os.path.join(current_save_dir, 'best_model.pth')\n",
    "    \n",
    "    if not os.path.exists(current_best_model_path):\n",
    "        print(f\" Model file not found at {current_best_model_path}\")\n",
    "        print(f\"   Make sure you have trained the {current_backbone} model first.\")\n",
    "        return\n",
    "\n",
    "    set_seed()\n",
    "    device = get_device()\n",
    "    \n",
    "    # Load test data ONLY\n",
    "    print(\"\\nLoading held-out test datasets...\")\n",
    "    d1_test_p, d1_test_b, d1_test_s = load_dataset1_split('test')\n",
    "    d2_test_p, d2_test_b, d2_test_s = load_dataset2_split('test')  # Now from merged+split data\n",
    "    \n",
    "    test_ds_combined = OralPathologyDataset(\n",
    "        d1_test_p + d2_test_p, \n",
    "        d1_test_b + d2_test_b, \n",
    "        d1_test_s + d2_test_s, \n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(test_ds_combined, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                            num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    print(f\"Test images: {len(test_ds_combined)}\")\n",
    "    print(f\"  - Dataset 1: {len(d1_test_p)}\")\n",
    "    print(f\"  - Dataset 2: {len(d2_test_p)}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nLoading model from {current_best_model_path}...\")\n",
    "    model = MultiTaskOralClassifier(backbone=current_backbone).to(device)\n",
    "    model.load_state_dict(torch.load(current_best_model_path))\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating on held-out test set...\")\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BINARY CLASSIFICATION RESULTS (Benign vs Malignant)\")\n",
    "    print(\"=\"*60)\n",
    "    acc_b = accuracy_score(results['targets_binary'], results['preds_binary'])\n",
    "    prec_b = precision_score(results['targets_binary'], results['preds_binary'], average='weighted', zero_division=0)\n",
    "    rec_b = recall_score(results['targets_binary'], results['preds_binary'], average='weighted', zero_division=0)\n",
    "    f1_b = f1_score(results['targets_binary'], results['preds_binary'], average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy:  {acc_b:.4f}\")\n",
    "    print(f\"Precision: {prec_b:.4f}\")\n",
    "    print(f\"Recall:    {rec_b:.4f}\")\n",
    "    print(f\"F1-Score:  {f1_b:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUBTYPE CLASSIFICATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    acc_s = accuracy_score(results['targets_subtype'], results['preds_subtype'])\n",
    "    prec_s = precision_score(results['targets_subtype'], results['preds_subtype'], average='weighted', zero_division=0)\n",
    "    rec_s = recall_score(results['targets_subtype'], results['preds_subtype'], average='weighted', zero_division=0)\n",
    "    f1_s = f1_score(results['targets_subtype'], results['preds_subtype'], average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy:  {acc_s:.4f}\")\n",
    "    print(f\"Precision: {prec_s:.4f}\")\n",
    "    print(f\"Recall:    {rec_s:.4f}\")\n",
    "    print(f\"F1-Score:  {f1_s:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-Class Report:\")\n",
    "    print(classification_report(results['targets_subtype'], results['preds_subtype'], \n",
    "                                 target_names=DS2_CLASSES, zero_division=0))\n",
    "    \n",
    "    # Save results to file\n",
    "    results_file = os.path.join(current_save_dir, 'evaluation_results.txt')\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"BINARY CLASSIFICATION RESULTS (Benign vs Malignant)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"Accuracy:  {acc_b:.4f}\\n\")\n",
    "        f.write(f\"Precision: {prec_b:.4f}\\n\")\n",
    "        f.write(f\"Recall:    {rec_b:.4f}\\n\")\n",
    "        f.write(f\"F1-Score:  {f1_b:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUBTYPE CLASSIFICATION RESULTS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"Accuracy:  {acc_s:.4f}\\n\")\n",
    "        f.write(f\"Precision: {prec_s:.4f}\\n\")\n",
    "        f.write(f\"Recall:    {rec_s:.4f}\\n\")\n",
    "        f.write(f\"F1-Score:  {f1_s:.4f}\\n\\n\")\n",
    "        f.write(\"Per-Class Report:\\n\")\n",
    "        f.write(classification_report(results['targets_subtype'], results['preds_subtype'], \n",
    "                                      target_names=DS2_CLASSES, zero_division=0))\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to {results_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
